{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cbc24c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.19.5 in c:\\users\\lawrence\\anaconda3\\lib\\site-packages (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.19.5 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb7c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "#https://github.com/chongyangma/cs231n/blob/master/assignments/assignment3/style_transfer_pytorch.py\n",
    "class TVLoss(nn.Module):\n",
    "    def __init__(self, TVLoss_weight= 1):\n",
    "        super(TVLoss,self).__init__()\n",
    "        self.TVLoss_weight = TVLoss_weight\n",
    "\n",
    "    def forward(self,x):    \n",
    "        w_variance = torch.sum(torch.pow(x[:,:,:,:-1] - x[:,:,:,1:], 2))\n",
    "        h_variance = torch.sum(torch.pow(x[:,:,:-1,:] - x[:,:,1:,:], 2))\n",
    "        loss = self.TVLoss_weight * (h_variance + w_variance)\n",
    "        return loss\n",
    "\n",
    "#https://github.com/pytorch/pytorch/issues/9160#issuecomment-483048684\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "#https://github.com/aitorzip/PyTorch-CycleGAN/blob/master/models.py\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,in_features):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        conv_block = [  nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        nn.InstanceNorm2d(in_features),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        nn.InstanceNorm2d(in_features)  ]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x) #skip connection\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_nc, ngf=64):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        #Inital Conv Block\n",
    "        model = [   nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(in_nc, ngf, 7),\n",
    "                    nn.InstanceNorm2d(ngf),\n",
    "                    nn.ReLU(inplace=True) ]\n",
    "\n",
    "        in_features = ngf\n",
    "        out_features = in_features *2\n",
    "\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #Return batch w/ encoded content picture\n",
    "        return [self.model(x['content']), x['style_label']]\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_nc, ngf, n_residual_blocks=5):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        in_features = ngf * 4\n",
    "        out_features = in_features//2\n",
    "\n",
    "        model = []\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            model += [  nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                        nn.InstanceNorm2d(out_features),\n",
    "                        nn.ReLU(inplace=True) ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features//2\n",
    "\n",
    "        # Output layer\n",
    "        model += [  nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(64, out_nc, 7),\n",
    "                    nn.Tanh() ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,n_styles, ngf, auto_id=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.t = nn.ModuleList([ResidualBlock(ngf*4) for i in range(n_styles)])\n",
    "        if auto_id:\n",
    "            self.t.append(Identity())\n",
    "\n",
    "    def forward(self,x):\n",
    "        #x0 is content, x[1][0] is label\n",
    "        label = x[1][0]\n",
    "        mix = np.sum([self.t[i](x[0])*v for (i,v) in enumerate(label) if v])\n",
    "        #return content transformed by style specific residual block\n",
    "        return mix\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,in_nc,out_nc,n_styles,ngf):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(in_nc,ngf)\n",
    "        self.transformer = Transformer(n_styles,ngf)\n",
    "        self.decoder = Decoder(out_nc,ngf)\n",
    "\n",
    "    def forward(self,x):\n",
    "        e = self.encoder(x)\n",
    "        t = self.transformer(e)\n",
    "        d = self.decoder(t)\n",
    "        return d\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Patch-Gan discriminator \n",
    "    \"\"\"\n",
    "    def __init__(self, in_nc, n_styles, ndf=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # A bunch of convolutions \n",
    "        model = [   nn.Conv2d(in_nc, 256, 4, stride=4),\n",
    "                    nn.ReLU()]\n",
    "\n",
    "        model += [  nn.Conv2d(256, 512, 2, stride=2),\n",
    "                    #nn.LeakyReLU(0.2, inplace=True)\n",
    "                    nn.ReLU()]\n",
    "        \n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "        # GAN (real/notreal) Output-\n",
    "        self.fldiscriminator = nn.Conv2d(512, 1, 1,stride=1, padding = 0)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.pool = nn.AvgPool2d(1)\n",
    "\n",
    "        # Classification Output\n",
    "        self.aux_clf = nn.Conv2d(512, n_styles, 1, padding = 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        base =  self.model(x)\n",
    "        # input : [1, 3, 128, 128]\n",
    "        # output: [1, 512, 16, 16]\n",
    "        # discrim: [1, 1, 17, 17]; clf: [1, 17, 17, 4]\n",
    "        discrim = self.fldiscriminator(base)\n",
    "        discrim = self.sig(discrim)\n",
    "        discrim = self.pool(discrim)\n",
    "        clf = self.aux_clf(base).transpose(1,3)\n",
    "\n",
    "        return [discrim,clf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b7a36bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.5\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d41da0eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9963 80 Compose(\n",
      "    Resize(size=143, interpolation=bicubic, max_size=None, antialias=None)\n",
      "    RandomCrop(size=(128, 128), padding=None)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_img, root_style, transforms_=None, mode='train'):\n",
    "        transforms_ = [ transforms.Resize(int(143), InterpolationMode.BICUBIC), \n",
    "                transforms.RandomCrop(128), \n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) \n",
    "              ]\n",
    "        #content source\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.X = sorted(glob.glob(os.path.join(root_img, '*')))\n",
    "        #style image source(s)\n",
    "        self.Y = []\n",
    "        style_sources = sorted(glob.glob(os.path.join(root_style, '*')))\n",
    "        for label, style in enumerate(style_sources):\n",
    "            temp = [(label, x) for x in sorted(glob.glob(style_sources[label]+\"/*\"))]\n",
    "            self.Y.extend(temp)\n",
    "    def __len__(self):\n",
    "        return max(len(self.X), len(self.Y))\n",
    "    def __getitem__(self, index):                                    \n",
    "        output = {}\n",
    "        output['content'] = self.transform(Image.open(self.X[index % len(self.X)]))\n",
    "        #select style\n",
    "        selection = self.Y[random.randint(0, len(self.Y) - 1)]\n",
    "        try:\n",
    "            output['style'] = self.transform(Image.open(selection[1]))\n",
    "        except:\n",
    "            selection = self.Y[random.randint(0, len(self.Y) - 1)]\n",
    "            output['style'] = self.transform(Image.open(selection[1]))\n",
    "            # print('thisuns grey')\n",
    "            # print(selection)\n",
    "        output['style_label'] = selection[0]\n",
    "        return output\n",
    "from torch.utils.data import DataLoader\n",
    "root = ImageDataset('./VOCdevkit/VOC2007/JPEGImages', './Gated_patch_voc2007/trainB')\n",
    "print(len(root.X), len(root.Y), root.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371b66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba71fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(root, batch_size=1, shuffle=True)\n",
    "batch = next(iter(dataloader))\n",
    "print(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "753ad693",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0,1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size-1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return Variable(torch.cat(to_return))\n",
    "\n",
    "def label2tensor(label,tensor):\n",
    "    for i in range(label.size(0)):\n",
    "        tensor[i].fill_(label[i])\n",
    "    return tensor\n",
    "\n",
    "# def tensor2image(tensor):\n",
    "#     image = 127.5*(tensor[0].cpu().float().numpy() + 1.0)\n",
    "#     if image.shape[0] == 1:\n",
    "#         image = np.tile(image, (3,1,1))\n",
    "#     return image.astype(np.uint8)\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant(m.bias.data, 0.0)\n",
    "\n",
    "class LambdaLR():\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert ((n_epochs - decay_start_epoch) > 0), \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107220b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "Discriminator loss: 14.162635803222656 Generator loss: 13.963260650634766\n",
      "Batch: 20\n",
      "Discriminator loss: 14.413558959960938 Generator loss: 14.272843360900879\n",
      "Batch: 40\n",
      "Discriminator loss: 10.688887596130371 Generator loss: 13.021153450012207\n",
      "Batch: 60\n",
      "Discriminator loss: 13.797469139099121 Generator loss: 13.379288673400879\n",
      "Batch: 80\n",
      "Discriminator loss: 11.066908836364746 Generator loss: 10.75394344329834\n",
      "Batch: 100\n",
      "Discriminator loss: 8.040660858154297 Generator loss: 8.807954788208008\n",
      "Batch: 120\n",
      "Discriminator loss: 6.967514991760254 Generator loss: 5.838262557983398\n",
      "Batch: 140\n",
      "Discriminator loss: 8.105093955993652 Generator loss: 7.01444673538208\n",
      "Batch: 160\n",
      "Discriminator loss: 10.052586555480957 Generator loss: 8.930877685546875\n",
      "Batch: 180\n",
      "Discriminator loss: 11.936324119567871 Generator loss: 9.769805908203125\n",
      "Batch: 200\n",
      "Discriminator loss: 15.019433975219727 Generator loss: 15.117688179016113\n",
      "Batch: 220\n",
      "Discriminator loss: 9.412156105041504 Generator loss: 8.64853572845459\n",
      "Batch: 240\n",
      "Discriminator loss: 7.6693644523620605 Generator loss: 5.736710071563721\n",
      "Batch: 260\n",
      "Discriminator loss: 6.478837013244629 Generator loss: 5.423378944396973\n",
      "Batch: 280\n",
      "Discriminator loss: 6.215019702911377 Generator loss: 5.326255798339844\n",
      "Batch: 300\n",
      "Discriminator loss: 7.309919357299805 Generator loss: 5.6067070960998535\n",
      "Batch: 320\n",
      "Discriminator loss: 6.683619976043701 Generator loss: 5.791006565093994\n",
      "Batch: 340\n",
      "Discriminator loss: 10.830310821533203 Generator loss: 11.927234649658203\n",
      "Batch: 360\n",
      "Discriminator loss: 8.962870597839355 Generator loss: 7.300961494445801\n",
      "Batch: 380\n",
      "Discriminator loss: 12.024125099182129 Generator loss: 9.354168891906738\n",
      "Batch: 400\n",
      "Discriminator loss: 7.4397101402282715 Generator loss: 5.579443454742432\n",
      "Batch: 420\n",
      "Discriminator loss: 7.744968414306641 Generator loss: 5.562103271484375\n",
      "Batch: 440\n",
      "Discriminator loss: 7.612236499786377 Generator loss: 6.3103766441345215\n",
      "Batch: 460\n",
      "Discriminator loss: 7.515992641448975 Generator loss: 6.029873847961426\n",
      "Batch: 480\n",
      "Discriminator loss: 6.950563430786133 Generator loss: 5.45376443862915\n",
      "Batch: 500\n",
      "Discriminator loss: 8.190807342529297 Generator loss: 6.225255966186523\n",
      "Batch: 520\n",
      "Discriminator loss: 7.418221950531006 Generator loss: 5.300512790679932\n",
      "Batch: 540\n",
      "Discriminator loss: 12.596613883972168 Generator loss: 6.089434623718262\n",
      "Batch: 560\n",
      "Discriminator loss: 7.1023149490356445 Generator loss: 5.824090957641602\n",
      "Batch: 580\n",
      "Discriminator loss: 6.273494243621826 Generator loss: 11.547685623168945\n",
      "Batch: 600\n",
      "Discriminator loss: 6.510082244873047 Generator loss: 5.779660701751709\n",
      "Batch: 620\n",
      "Discriminator loss: 6.891748905181885 Generator loss: 5.367264747619629\n",
      "Batch: 640\n",
      "Discriminator loss: 8.914935111999512 Generator loss: 5.613311290740967\n",
      "Batch: 660\n",
      "Discriminator loss: 6.3355278968811035 Generator loss: 5.428489685058594\n",
      "Batch: 680\n",
      "Discriminator loss: 6.510620594024658 Generator loss: 5.537303924560547\n",
      "Batch: 700\n",
      "Discriminator loss: 9.051976203918457 Generator loss: 5.515683174133301\n",
      "Batch: 720\n",
      "Discriminator loss: 7.830146312713623 Generator loss: 5.683990955352783\n",
      "Batch: 740\n",
      "Discriminator loss: 7.4037604331970215 Generator loss: 5.649499893188477\n",
      "Batch: 760\n",
      "Discriminator loss: 7.609670639038086 Generator loss: 5.511610984802246\n",
      "Batch: 780\n",
      "Discriminator loss: 5.8882036209106445 Generator loss: 5.48646354675293\n",
      "Batch: 800\n",
      "Discriminator loss: 8.266355514526367 Generator loss: 5.567031383514404\n",
      "Batch: 820\n",
      "Discriminator loss: 10.88513469696045 Generator loss: 5.5526957511901855\n",
      "Batch: 840\n",
      "Discriminator loss: 6.867131233215332 Generator loss: 5.480268955230713\n",
      "Batch: 860\n",
      "Discriminator loss: 5.791299343109131 Generator loss: 5.422455310821533\n",
      "Batch: 880\n",
      "Discriminator loss: 6.819643974304199 Generator loss: 5.482206344604492\n",
      "Batch: 900\n",
      "Discriminator loss: 9.335122108459473 Generator loss: 5.391927242279053\n",
      "Batch: 920\n",
      "Discriminator loss: 7.036120891571045 Generator loss: 5.510179042816162\n",
      "Batch: 940\n",
      "Discriminator loss: 6.4336771965026855 Generator loss: 5.365137100219727\n",
      "Batch: 960\n",
      "Discriminator loss: 8.722567558288574 Generator loss: 5.493834018707275\n",
      "Batch: 980\n",
      "Discriminator loss: 6.8706560134887695 Generator loss: 5.520294666290283\n",
      "Batch: 1000\n",
      "Discriminator loss: 11.29418659210205 Generator loss: 5.687061786651611\n",
      "Batch: 1020\n",
      "Discriminator loss: 8.671406745910645 Generator loss: 5.56860876083374\n",
      "Batch: 1040\n",
      "Discriminator loss: 6.9028239250183105 Generator loss: 5.512911796569824\n",
      "Batch: 1060\n",
      "Discriminator loss: 6.460292816162109 Generator loss: 5.573272705078125\n",
      "Batch: 1080\n",
      "Discriminator loss: 11.215863227844238 Generator loss: 10.155811309814453\n",
      "Batch: 1100\n",
      "Discriminator loss: 8.946111679077148 Generator loss: 7.399317264556885\n",
      "Batch: 1120\n",
      "Discriminator loss: 7.2486724853515625 Generator loss: 6.378328323364258\n",
      "Batch: 1140\n",
      "Discriminator loss: 9.686932563781738 Generator loss: 6.848877429962158\n",
      "Batch: 1160\n",
      "Discriminator loss: 6.234223365783691 Generator loss: 5.631932735443115\n",
      "Batch: 1180\n",
      "Discriminator loss: 13.027265548706055 Generator loss: 6.56674861907959\n",
      "Batch: 1200\n",
      "Discriminator loss: 10.514019012451172 Generator loss: 9.558267593383789\n",
      "Batch: 1220\n",
      "Discriminator loss: 10.23157787322998 Generator loss: 6.797800064086914\n",
      "Batch: 1240\n",
      "Discriminator loss: 6.745270252227783 Generator loss: 5.858366966247559\n",
      "Batch: 1260\n",
      "Discriminator loss: 6.444216251373291 Generator loss: 5.765817642211914\n",
      "Batch: 1280\n",
      "Discriminator loss: 8.362954139709473 Generator loss: 5.953484058380127\n",
      "Batch: 1300\n",
      "Discriminator loss: 5.6592230796813965 Generator loss: 5.5798516273498535\n",
      "Batch: 1320\n",
      "Discriminator loss: 5.89196252822876 Generator loss: 5.848942279815674\n",
      "Batch: 1340\n",
      "Discriminator loss: 7.117300033569336 Generator loss: 5.614801406860352\n",
      "Batch: 1360\n",
      "Discriminator loss: 7.703588008880615 Generator loss: 5.5840983390808105\n",
      "Batch: 1380\n",
      "Discriminator loss: 6.560161113739014 Generator loss: 5.704143047332764\n",
      "Batch: 1400\n",
      "Discriminator loss: 5.735306262969971 Generator loss: 5.546843528747559\n",
      "Batch: 1420\n",
      "Discriminator loss: 9.215274810791016 Generator loss: 5.535232067108154\n",
      "Batch: 1440\n",
      "Discriminator loss: 6.710458278656006 Generator loss: 5.571702003479004\n",
      "Batch: 1460\n",
      "Discriminator loss: 10.20041275024414 Generator loss: 5.714066505432129\n",
      "Batch: 1480\n",
      "Discriminator loss: 6.0982489585876465 Generator loss: 5.659510135650635\n",
      "Batch: 1500\n",
      "Discriminator loss: 6.557272911071777 Generator loss: 5.545863151550293\n",
      "Batch: 1520\n",
      "Discriminator loss: 9.496182441711426 Generator loss: 5.729621410369873\n",
      "Batch: 1540\n",
      "Discriminator loss: 6.855588912963867 Generator loss: 5.556615352630615\n",
      "Batch: 1560\n",
      "Discriminator loss: 6.226786136627197 Generator loss: 5.466536521911621\n",
      "Batch: 1580\n",
      "Discriminator loss: 10.512435913085938 Generator loss: 5.634427547454834\n",
      "Batch: 1600\n",
      "Discriminator loss: 8.70777702331543 Generator loss: 5.729704856872559\n",
      "Batch: 1620\n",
      "Discriminator loss: 8.19462776184082 Generator loss: 5.63896369934082\n",
      "Batch: 1640\n",
      "Discriminator loss: 6.4638352394104 Generator loss: 5.582188129425049\n",
      "Batch: 1660\n",
      "Discriminator loss: 5.690872669219971 Generator loss: 5.580727577209473\n",
      "Batch: 1680\n",
      "Discriminator loss: 7.723776340484619 Generator loss: 5.355937480926514\n",
      "Batch: 1700\n",
      "Discriminator loss: 5.754207611083984 Generator loss: 5.478911399841309\n",
      "Batch: 1720\n",
      "Discriminator loss: 6.058409690856934 Generator loss: 5.644306182861328\n",
      "Batch: 1740\n",
      "Discriminator loss: 7.781871318817139 Generator loss: 5.550997257232666\n",
      "Batch: 1760\n",
      "Discriminator loss: 7.417686939239502 Generator loss: 17.242382049560547\n",
      "Batch: 1780\n",
      "Discriminator loss: 6.275156497955322 Generator loss: 6.405647277832031\n",
      "Batch: 1800\n",
      "Discriminator loss: 6.892207145690918 Generator loss: 6.3469624519348145\n",
      "Batch: 1820\n",
      "Discriminator loss: 5.991850852966309 Generator loss: 6.012599945068359\n",
      "Batch: 1840\n",
      "Discriminator loss: 5.804537296295166 Generator loss: 5.79702615737915\n",
      "Batch: 1860\n",
      "Discriminator loss: 5.682223320007324 Generator loss: 5.878459453582764\n",
      "Batch: 1880\n",
      "Discriminator loss: 8.21374797821045 Generator loss: 5.82784366607666\n",
      "Batch: 1900\n",
      "Discriminator loss: 6.253379821777344 Generator loss: 5.889839172363281\n",
      "Batch: 1920\n",
      "Discriminator loss: 6.134413719177246 Generator loss: 5.72041654586792\n",
      "Batch: 1940\n",
      "Discriminator loss: 6.627262592315674 Generator loss: 5.940020561218262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1960\n",
      "Discriminator loss: 6.226527214050293 Generator loss: 5.91234827041626\n",
      "Batch: 1980\n",
      "Discriminator loss: 5.648400783538818 Generator loss: 5.976004600524902\n",
      "Batch: 2000\n",
      "Discriminator loss: 7.083575248718262 Generator loss: 5.740179061889648\n",
      "Batch: 2020\n",
      "Discriminator loss: 6.060420513153076 Generator loss: 5.652252197265625\n",
      "Batch: 2040\n",
      "Discriminator loss: 7.406171798706055 Generator loss: 5.904869079589844\n",
      "Batch: 2060\n",
      "Discriminator loss: 5.869866847991943 Generator loss: 5.529762268066406\n",
      "Batch: 2080\n",
      "Discriminator loss: 5.735332489013672 Generator loss: 5.459377765655518\n",
      "Batch: 2100\n",
      "Discriminator loss: 6.81845235824585 Generator loss: 5.674100399017334\n",
      "Batch: 2120\n",
      "Discriminator loss: 9.379009246826172 Generator loss: 5.796261310577393\n",
      "Batch: 2140\n",
      "Discriminator loss: 5.353666305541992 Generator loss: 5.522758960723877\n",
      "Batch: 2160\n",
      "Discriminator loss: 6.169582843780518 Generator loss: 5.509429931640625\n",
      "Batch: 2180\n",
      "Discriminator loss: 7.250353813171387 Generator loss: 5.762458801269531\n",
      "Batch: 2200\n",
      "Discriminator loss: 6.041713714599609 Generator loss: 5.679993152618408\n",
      "Batch: 2220\n",
      "Discriminator loss: 5.648146629333496 Generator loss: 5.795581340789795\n",
      "Batch: 2240\n",
      "Discriminator loss: 5.672783374786377 Generator loss: 5.756143093109131\n",
      "Batch: 2260\n",
      "Discriminator loss: 5.761960983276367 Generator loss: 5.589366912841797\n",
      "Batch: 2280\n",
      "Discriminator loss: 8.595576286315918 Generator loss: 5.706413745880127\n",
      "Batch: 2300\n",
      "Discriminator loss: 5.84993839263916 Generator loss: 5.861076354980469\n",
      "Batch: 2320\n",
      "Discriminator loss: 7.266790866851807 Generator loss: 6.139616966247559\n",
      "Batch: 2340\n",
      "Discriminator loss: 6.395081043243408 Generator loss: 6.609808921813965\n",
      "Batch: 2360\n",
      "Discriminator loss: 6.944133281707764 Generator loss: 8.257802963256836\n",
      "Batch: 2380\n",
      "Discriminator loss: 8.832261085510254 Generator loss: 5.828805923461914\n",
      "Batch: 2400\n",
      "Discriminator loss: 10.369766235351562 Generator loss: 5.933185577392578\n",
      "Batch: 2420\n",
      "Discriminator loss: 6.064142227172852 Generator loss: 5.609358787536621\n",
      "Batch: 2440\n",
      "Discriminator loss: 6.536161422729492 Generator loss: 5.71213960647583\n",
      "Batch: 2460\n",
      "Discriminator loss: 6.2970428466796875 Generator loss: 5.775564670562744\n",
      "Batch: 2480\n",
      "Discriminator loss: 6.871542453765869 Generator loss: 5.721324443817139\n"
     ]
    }
   ],
   "source": [
    "#TRAIN OPTIONS FROM GATED GAN\n",
    "epoch = 0\n",
    "n_epochs = 16\n",
    "decay_epoch=1\n",
    "batchSize = 1\n",
    "dataroot = './photo2fourcollection'\n",
    "loadSize = 143\n",
    "fineSize = 128\n",
    "ngf = 64\n",
    "ndf = 64    \n",
    "in_nc = 3 \n",
    "out_nc = 3 \n",
    "lr = 0.0002 \n",
    "gpu = 1 \n",
    "lambda_A = 10.0\n",
    "pool_size = 50\n",
    "resize_or_crop = 'resize_and_crop'\n",
    "autoencoder_constrain = 10 \n",
    "n_styles = 4\n",
    "cuda=True\n",
    "tv_strength=2e-6\n",
    "\n",
    "generator = Generator(in_nc, out_nc, n_styles, ngf)\n",
    "discriminator= Discriminator(in_nc, n_styles, ndf)\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "#Losses Init\n",
    "use_lsgan=True\n",
    "if use_lsgan:\n",
    "    criterion_GAN = nn.MSELoss()\n",
    "else: \n",
    "    criterion_GAN = nn.BCELoss()\n",
    "    \n",
    "\n",
    "criterion_ACGAN = nn.CrossEntropyLoss()\n",
    "criterion_Rec = nn.L1Loss()\n",
    "criterion_TV = TVLoss(TVLoss_weight=tv_strength)\n",
    "\n",
    "#Optimizers & LR schedulers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(),\n",
    "                                lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), \n",
    "                               lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch,decay_epoch).step)\n",
    "lr_scheduler_D = torch.optim.lr_scheduler.LambdaLR(optimizer_D, lr_lambda=LambdaLR(n_epochs,epoch, decay_epoch).step)\n",
    "\n",
    "#Set vars for training\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "input_A = Tensor(batchSize, in_nc, fineSize, fineSize)\n",
    "input_B = Tensor(batchSize, out_nc, fineSize, fineSize)\n",
    "target_real = Variable(Tensor(batchSize).fill_(1.0), requires_grad=False)\n",
    "target_fake = Variable(Tensor(batchSize).fill_(0.0), requires_grad=False)\n",
    "\n",
    "D_A_size = discriminator(input_A.copy_(batch['style']))[0].size()  \n",
    "D_AC_size = discriminator(input_B.copy_(batch['style']))[1].size()\n",
    "\n",
    "class_label_B = Tensor(D_AC_size[0],D_AC_size[1],D_AC_size[2]).long()\n",
    "\n",
    "autoflag_OHE = Tensor(1,n_styles+1).fill_(0).long()\n",
    "autoflag_OHE[0][-1] = 1\n",
    "\n",
    "fake_label = Tensor(D_A_size).fill_(0.0)\n",
    "real_label = Tensor(D_A_size).fill_(0.99) \n",
    "\n",
    "rec_A_AE = Tensor(batchSize,in_nc,fineSize,fineSize)\n",
    "\n",
    "fake_buffer = ReplayBuffer()\n",
    "\n",
    "##Init Weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "\n",
    "\n",
    "### TRAIN LOOP\n",
    "for epoch in range(epoch, n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        ## Unpack minibatch\n",
    "        # source content\n",
    "        real_content = Variable(input_A.copy_(batch['content']))\n",
    "        # target style\n",
    "        real_style = Variable(input_B.copy_(batch['style']))\n",
    "        # style label\n",
    "        style_label = batch['style_label']\n",
    "        # one-hot encoded style\n",
    "        style_OHE = F.one_hot(style_label,n_styles).long()\n",
    "        # style Label mapped over 1x19x19 tensor for patch discriminator \n",
    "        class_label = class_label_B.copy_(label2tensor(style_label,class_label_B)).long()\n",
    "        \n",
    "        #### Update Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Generate style-transfered image\n",
    "        genfake = generator({\n",
    "            'content':real_content,\n",
    "            'style_label': style_OHE})\n",
    "        \n",
    "        # Add generated image to image pool and randomly sample pool \n",
    "        fake = fake_buffer.push_and_pop(genfake)\n",
    "        # Discriminator forward pass with sampled fake \n",
    "        out_gan, out_class = discriminator(fake)\n",
    "        # Discriminator Fake loss (correctly identify generated images)\n",
    "        errD_fake = criterion_GAN(out_gan, fake_label)\n",
    "        # Backward pass and parameter optimization\n",
    "        errD_fake.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "        # Discriminator forward pass with target style\n",
    "        out_gan, out_class = discriminator(real_style)\n",
    "        # Discriminator Style Classification loss\n",
    "        errD_real_class = criterion_ACGAN(out_class.transpose(1,3),class_label)*lambda_A\n",
    "        # Discriminator Real loss (correctly identify real style images)\n",
    "        errD_real = criterion_GAN(out_gan, real_label)        \n",
    "        errD_real_total = errD_real + errD_real_class\n",
    "        # Backward pass and parameter optimization\n",
    "        errD_real_total.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        \n",
    "        errD = (errD_real+errD_fake)/2.0\n",
    "        \n",
    "                \n",
    "        #### Generator Update\n",
    "        ## Style Transfer Loss\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Discriminator forward pass with generated style transfer\n",
    "        out_gan, out_class = discriminator(genfake)\n",
    "        \n",
    "        # Generator gan (real/fake) loss\n",
    "        err_gan = criterion_GAN(out_gan, real_label)\n",
    "        # Generator style class loss\n",
    "        err_class = criterion_ACGAN(out_class.transpose(1,3), class_label)*lambda_A\n",
    "        # Total Variation loss\n",
    "        err_TV = criterion_TV(genfake)\n",
    "        \n",
    "        errG_tot = err_gan + err_class + err_TV\n",
    "        errG_tot.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        ## Auto-Encoder (Recreation) Loss\n",
    "        optimizer_G.zero_grad()\n",
    "        identity = generator({\n",
    "            'content': real_content,\n",
    "            'style_label': autoflag_OHE,\n",
    "        })\n",
    "        err_ae = criterion_Rec(identity,real_content)*autoencoder_constrain\n",
    "        err_ae.backward()\n",
    "        optimizer_G.step()\n",
    "        if i % 20 == 0:\n",
    "            print('Batch:', i)\n",
    "            print(\"Discriminator loss:\", errD_real_total.item(), \"Generator loss:\", errG_tot.item())\n",
    "\n",
    "        \n",
    "    \n",
    "    ##update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D.step()\n",
    "    \n",
    "    #Save model\n",
    "    torch.save(generator.state_dict(), 'output/netG.pth')\n",
    "    torch.save(discriminator.state_dict(), 'output/netD.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd665d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
